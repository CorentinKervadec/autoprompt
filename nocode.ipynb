{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "understood-coach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stunning-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, AutoConfig, AutoTokenizer, AutoModelForMaskedLM\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "czech-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoprompt.utils as utils\n",
    "from autoprompt.models import ContinuousTriggerMLM\n",
    "from autoprompt.preprocessors import PREPROCESSORS\n",
    "from autoprompt.evaluators import MLM_EVALUATORS\n",
    "\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "enormous-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoprompt import continuous_trigger_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "objective-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_config = utils.distributed_setup(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amino-pioneer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-large and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config, tokenizer, base_model = continuous_trigger_mlm.load_transformers(\"roberta-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "mighty-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = json.loads(\"\"\"{\"plural\": \"plural\", \"singular\": \"singular\"}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "regional-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"[T] [T] Q: {text} [P]\"\n",
    "templatizer = utils.MultiTokenTemplatizer(\n",
    "        template=template,\n",
    "        tokenizer=tokenizer,\n",
    "        label_field=\"label\",\n",
    "        label_map=label_map,\n",
    "        add_padding=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "effective-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_trigger_ids(initial_trigger, tokenizer):\n",
    "    \"\"\"Converts a list of trigger tokens to a tensor of trigger token ids.\"\"\"\n",
    "    if initial_trigger is None:\n",
    "        return\n",
    "    initial_trigger_ids = torch.tensor(\n",
    "        tokenizer.convert_tokens_to_ids(initial_trigger)\n",
    "    )\n",
    "    detokenized = tokenizer.convert_ids_to_tokens(initial_trigger_ids)\n",
    "    return initial_trigger_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-diagram",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "neither-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_trigger_ids = get_initial_trigger_ids(['?', '|', ','], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "declared-planning",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-a2a7d2c37195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mbase_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mnum_trigger_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemplatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_trigger_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0minitial_trigger_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_trigger_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistributed_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/autoprompt/autoprompt/models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_model, num_trigger_tokens, initial_trigger_ids)\u001b[0m\n\u001b[1;32m     44\u001b[0m         )\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_trigger_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_trigger_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "model = ContinuousTriggerMLM(\n",
    "        base_model=base_model,\n",
    "        num_trigger_tokens=templatizer.num_trigger_tokens,\n",
    "        initial_trigger_ids=initial_trigger_ids,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "parental-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampler(\n",
    "    dataset,\n",
    "    evaluation_strategy,\n",
    "    distributed_config,\n",
    "    train=False\n",
    "):\n",
    "    \"\"\"Sets up the correct sampler for a data loader.\"\"\"\n",
    "    # Sampling is handled by data iterator for multiple choice problems.\n",
    "    if evaluation_strategy != 'classification':\n",
    "        return\n",
    "    # Multi-GPU training\n",
    "    if distributed_config.world_size != -1:\n",
    "        return torch.utils.data.DistributedSampler(dataset, shuffle=train)\n",
    "    # Defaults\n",
    "    if train:\n",
    "        return torch.utils.data.RandomSampler(dataset)\n",
    "    else:\n",
    "        return torch.utils.data.SequentialSampler(dataset)\n",
    "\n",
    "\n",
    "def _stringify(d):\n",
    "    return {k: str(v) for k, v in d.items()}\n",
    "\n",
    "def preprocess_jsonl_string(string, **kwargs):\n",
    "    for line in string.readlines():\n",
    "        yield _stringify(json.loads(line))\n",
    "\n",
    "def load_trigger_dataset(\n",
    "    fname,\n",
    "    templatizer,\n",
    "    limit=None,\n",
    "    train=False,\n",
    "    preprocessor_key=None,\n",
    "):\n",
    "    instances = []\n",
    "    for x in preprocess_jsonl_string(fname, train=train):\n",
    "        try:\n",
    "            model_inputs, label_id = templatizer(x, train=train)\n",
    "        except ValueError as e:\n",
    "            logger.warning('Encountered error \"%s\" when processing \"%s\".  Skipping.', e, x)\n",
    "            continue\n",
    "        else:\n",
    "            instances.append((model_inputs, label_id))\n",
    "    if limit:\n",
    "        limit = min(len(instances), limit)\n",
    "        return random.sample(instances, limit)\n",
    "    else:\n",
    "        return instances\n",
    "\n",
    "\n",
    "\n",
    "def load_datasets(templatizer, distributed_config, evaluation_strategy, \n",
    "                  train, dev, test, preprocessor, bsz=2, limit=None):\n",
    "    \"\"\"Loads the training, dev and test datasets.\"\"\"\n",
    "    dataset_constructor = load_trigger_dataset\n",
    "    collator = utils.Collator(pad_token_id=templatizer.pad_token_id)\n",
    "\n",
    "    train_dataset = dataset_constructor(\n",
    "        train,\n",
    "        templatizer=templatizer,\n",
    "        train=True,\n",
    "        preprocessor_key=preprocessor,\n",
    "        limit=limit,\n",
    "    )\n",
    "    train_sampler = get_sampler(train_dataset, evaluation_strategy, distributed_config, train=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=bsz, collate_fn=collator, sampler=train_sampler)\n",
    "\n",
    "    dev_dataset = dataset_constructor(\n",
    "        dev,\n",
    "        templatizer=templatizer,\n",
    "        preprocessor_key=preprocessor,\n",
    "        limit=limit,\n",
    "    )\n",
    "    dev_sampler = get_sampler(dev_dataset, evaluation_strategy, distributed_config, train=False)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=bsz, collate_fn=collator, sampler=dev_sampler)\n",
    "\n",
    "    test_dataset = dataset_constructor(\n",
    "        test,\n",
    "        templatizer=templatizer,\n",
    "        preprocessor_key=preprocessor,\n",
    "    )\n",
    "    test_sampler = get_sampler(test_dataset, evaluation_strategy, distributed_config, train=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=bsz, collate_fn=collator, sampler=test_sampler)\n",
    "\n",
    "    return train_loader, dev_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "modified-ghost",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plural'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map['plural']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "encouraging-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "evaluation_strategy = \"classification\"\n",
    "preprocessor = \"jsonl-string\"\n",
    "train_file = StringIO('{\"text\":\"beers\", \"label\":\"plural\"}\\n{\"text\":\"beer\", \"label\":\"singular\"}')\n",
    "dev_file = StringIO('{\"text\":\"beers\", \"label\":\"plural\"}\\n{\"text\":\"beer\", \"label\":\"singular\"}')\n",
    "test_file = StringIO('{\"text\":\"beers\", \"label\":\"plural\"}\\n{\"text\":\"beer\", \"label\":\"singular\"}')\n",
    "\n",
    "\n",
    "train_loader, dev_loader, test_loader = load_datasets(\n",
    "    templatizer=templatizer,\n",
    "    distributed_config=distributed_config,    \n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    train = train_file,\n",
    "    dev = dev_file,\n",
    "    test = test_file,\n",
    "    preprocessor=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "marine-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr, finetune_lr, finetune_mode='partial'):\n",
    "    \"\"\"Handles setting the optimizer up for different finetuning modes.\"\"\"\n",
    "    params = [{'params': [model.trigger_embeddings]}]\n",
    "    if finetune_mode == 'partial':\n",
    "        params.append({\n",
    "            'params': model.lm_head.parameters(),\n",
    "            'lr': finetune_lr if finetune_lr else lr\n",
    "        })\n",
    "    elif finetune_mode == 'all':\n",
    "        params.append({\n",
    "            'params': [p for p in model.parameters() if not torch.equal(p, model.trigger_embeddings)],\n",
    "            'lr': finetune_lr if finetune_lr else lr\n",
    "        })\n",
    "    return AdamW(\n",
    "        params,\n",
    "        lr=lr,\n",
    "        weight_decay=1e-2,\n",
    "        eps=1e-8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "chief-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_lr = 0.001\n",
    "finetune_mode = 'partial'\n",
    "lr = 0.1\n",
    "optimizer = get_optimizer(model, finetune_lr, lr, finetune_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "commercial-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    if isinstance(data, dict):\n",
    "        return {k: to_device(v, device) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "checked-dispute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "minus-festival",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [6, 1024] cannot be broadcast to indexing result of shape [4, 1024]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-55df953992fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         loss, correct, preds = evaluator(model_inputs, labels, train=True, \n\u001b[0;32m---> 43\u001b[0;31m                                 evaluation_metric=evaluation_metric)\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/autoprompt/autoprompt/evaluators.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, model_inputs, labels, train, evaluation_metric)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mpredict_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredict_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         predict_logits = torch.gather(\n\u001b[1;32m    243\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredict_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/autoprompt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/autoprompt/autoprompt/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, labels)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Insert trigger embeddings into input embeddings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0minputs_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrigger_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs_embeds'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [6, 1024] cannot be broadcast to indexing result of shape [4, 1024]"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "disable_dropout = False\n",
    "quiet = False\n",
    "evaluation_metric = \"accuracy\"\n",
    "accumulation_steps = 1\n",
    "decoding_strategy = None\n",
    "model.to(distributed_config.device)\n",
    "\n",
    "\n",
    "evaluator = MLM_EVALUATORS[evaluation_strategy](\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    label_map=label_map,\n",
    "    decoding_strategy=decoding_strategy,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if not disable_dropout:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    if distributed_config.is_main_process and not quiet:\n",
    "        iter_ = tqdm(train_loader)\n",
    "    else:\n",
    "        iter_ = train_loader\n",
    "    print(epoch)\n",
    "    total_loss = torch.tensor(0.0, device=distributed_config.device)\n",
    "    if evaluation_metric == 'accuracy':\n",
    "        total_correct = {'accuracy': torch.tensor(0.0, device=distributed_config.device)}\n",
    "    else:\n",
    "        total_correct = {'TP': torch.tensor(0.0, device=distributed_config.device),\n",
    "                         'FP': torch.tensor(0.0, device=distributed_config.device),\n",
    "                         'TN': torch.tensor(0.0, device=distributed_config.device),\n",
    "                         'FN': torch.tensor(0.0, device=distributed_config.device)}\n",
    "    denom = torch.tensor(0.0, device=distributed_config.device)\n",
    "    optimizer.zero_grad()\n",
    "    for i, (model_inputs, labels) in enumerate(iter_):\n",
    "        model_inputs = to_device(model_inputs, distributed_config.device)\n",
    "        labels = to_device(labels, distributed_config.device)\n",
    "        loss, correct, preds = evaluator(model_inputs, labels, train=True, \n",
    "                                evaluation_metric=evaluation_metric)\n",
    "        loss /= accumulation_steps\n",
    "        loss.backward()\n",
    "        if (i % accumulation_steps) == (accumulation_steps - 1):\n",
    "            logger.debug('Optimizer step.')\n",
    "            if args.clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # TODO: Metric logging is clumsy/brittle.\n",
    "        batch_size = 1.0 if evaluation_strategy == 'multiple-choice' else labels.size(0)\n",
    "        total_loss += loss.detach() * batch_size\n",
    "        for metric in correct:\n",
    "            total_correct[metric] += correct[metric].detach()\n",
    "        denom += batch_size\n",
    "\n",
    "        # NOTE: This loss/accuracy is only on the subset  of training data\n",
    "        # in the main process.\n",
    "        if distributed_config.is_main_process and not args.quiet:\n",
    "            if evaluation_metric == 'accuracy':\n",
    "                iter_.set_description(\n",
    "                    f'Loss: {total_loss / (denom + 1e-13): 0.4f}, '\n",
    "                    f\"Accuracy: {total_correct['accuracy'] / (denom + 1e-13): 0.4f}\"\n",
    "                )\n",
    "            elif evaluation_metric == 'MCC':\n",
    "                mcc_numerator = (total_correct['TP'] * total_correct['TN'] - \n",
    "                                total_correct['FP'] * total_correct['FN'])\n",
    "                mcc_denominator = torch.sqrt((total_correct['TP'] + total_correct['FP']) *\n",
    "                                             (total_correct['TP'] + total_correct['FN']) *\n",
    "                                             (total_correct['TN'] + total_correct['FP']) *\n",
    "                                             (total_correct['TN'] + total_correct['FN']))\n",
    "                iter_.set_description(\n",
    "                    f'Loss: {total_loss / (denom + 1e-13): 0.4f}, '\n",
    "                    f\"MCC: {mcc_numerator / mcc_denominator: 0.4f}\"    \n",
    "                )\n",
    "            elif evaluation_metric == 'F1':\n",
    "                precision = total_correct['TP'] / (total_correct['TP'] + total_correct['FP'])\n",
    "                recall = total_correct['TP'] / (total_correct['TP'] + total_correct['FN'])\n",
    "                f1 = (2 * precision * recall) / (precision + recall)\n",
    "                iter_.set_description(\n",
    "                    f'Loss: {total_loss / (denom + 1e-13): 0.4f}, '\n",
    "                    f\"F1 score: {f1: 0.4f}\"    \n",
    "                )\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoprompt",
   "language": "python",
   "name": "autoprompt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
